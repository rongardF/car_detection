{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 06:23:13.137027: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-27 06:23:13.138106: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-27 06:23:13.140562: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-08-27 06:23:13.147542: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-27 06:23:13.158385: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-27 06:23:13.161518: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-27 06:23:13.170479: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-27 06:23:13.748668: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "\n",
    "path_to_images = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/all_images\"\n",
    "train_images_path = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/train_images\"\n",
    "test_images_path = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/test_images\"\n",
    "path_to_annos_file = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/cars_train_annos.mat\"\n",
    "path_to_metadata_file = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/cars_meta.mat\"\n",
    "\n",
    "path_to_model = \"/home/eonrrfe/Documents/Repos/Others/CarDetection/test_images/reduced_stanford_dataset/resnet_50_model.keras\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUILD FOLDER STRUCTURE FOR TRAINING\n",
    "\n",
    "new_mat = scipy.io.loadmat(path_to_annos_file)\n",
    "metadata_mat = scipy.io.loadmat(path_to_metadata_file)\n",
    "trial = np.hstack(new_mat['annotations'])\n",
    "trial_df = pd.DataFrame(trial)\n",
    "trial_df['file_name'] = trial_df['fname']\n",
    "trial_df['file_name'] = trial_df['file_name'].map(np.array2string)\n",
    "trial_df['file_name'] = trial_df['file_name'].map(lambda x:x.lstrip(\"['\").rstrip(\"']\"))\n",
    "trial_df['class_number'] = trial_df['class'].astype(float)\n",
    "names= metadata_mat['class_names'].copy()\n",
    "names_df = pd.DataFrame(names).T\n",
    "names_df['class_number'] =names_df.index+1\n",
    "names_df['car_name'] = names_df[0].map(lambda x:np.array2string(x).lstrip(\"['\").rstrip(\"']\"))\n",
    "names_df = names_df.drop(columns = 0)\n",
    "names_df['brand'] = names_df['car_name'].map(lambda x:x.split()[0])\n",
    "names_df['model'] = names_df['car_name'].map(lambda x:x.split()[1])\n",
    "names_df['both']= names_df['brand'] +  names_df['model']\n",
    "\n",
    "\n",
    "names_df['both'] = names_df['both'].replace('RamC/V','RamCV')\n",
    "names_df['both'] = names_df['both'].map(lambda x : x.lower())\n",
    "carBrand = names_df['both'].unique()\n",
    "# names_df\n",
    "\n",
    "for i in carBrand:\n",
    "  try:\n",
    "    os.makedirs(f\"{train_images_path}/{i}\")\n",
    "  except FileExistsError:\n",
    "    pass\n",
    "\n",
    "for i in carBrand:\n",
    "  try:\n",
    "    os.makedirs(f\"{test_images_path}/{i}\")\n",
    "  except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPY FILES ACCORDING TO TRAIN-TEST SPLIT\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "merged_df_2 = trial_df.merge(names_df, left_on='class_number', right_on = 'class_number')\n",
    "# merged_df_2\n",
    "\n",
    "# test size here specifies how big percentage will be left for testing\n",
    "train, test = train_test_split(merged_df_2, test_size = 0.2, random_state = 42)\n",
    "# train\n",
    "\n",
    "for b in carBrand:\n",
    "  temp_df = train[train['both']==b]\n",
    "  temp_df.reset_index(drop = True)\n",
    "#   print(b)\n",
    "  for i in list(temp_df.index.values):\n",
    "    shutil.copy(f'{path_to_images}/'+temp_df['file_name'][i], f'{train_images_path}/'+b.lower()+'/'+temp_df['file_name'][i])\n",
    "\n",
    "\n",
    "for b in carBrand:\n",
    "  temp_df = test[test['both']==b]\n",
    "  temp_df.reset_index(drop = True)\n",
    "#   print(b)\n",
    "  for i in list(temp_df.index.values):\n",
    "    shutil.copy(f'{path_to_images}/'+temp_df['file_name'][i], f'{test_images_path}/'+b.lower()+'/'+temp_df['file_name'][i])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5264 images belonging to 157 classes.\n",
      "Found 1251 images belonging to 157 classes.\n"
     ]
    }
   ],
   "source": [
    "# PREPEAR FOR TRAINING THE RESNET50 MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "image_size = (224, 224)\n",
    "batch_size = 50\n",
    "transform = ImageDataGenerator(preprocessing_function= keras.applications.resnet.preprocess_input, validation_split=0.2)\n",
    "\n",
    "# we split the training dataset further to 80-20 for training and validating\n",
    "\n",
    "train_ds = transform.flow_from_directory(\n",
    "    train_images_path,\n",
    "    subset=\"training\", seed = 42,target_size=image_size,batch_size=batch_size\n",
    ")\n",
    "val_ds = transform.flow_from_directory(\n",
    "    train_images_path,\n",
    "    subset=\"validation\", seed = 42,target_size=image_size,batch_size=batch_size\n",
    ")\n",
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "inputs = keras.Input(shape=(224, 224, 3))\n",
    "base_model = ResNet50(weights='imagenet', include_top=True, input_tensor=inputs)\n",
    "\n",
    "last_layer = base_model.layers[-2].output \n",
    "out = Dense(units = 157, activation = 'softmax', name = 'ouput')(last_layer)\n",
    "new_base_model = Model(inputs = inputs, outputs = out)\n",
    "\n",
    "for layer in new_base_model.layers[:-25]:\n",
    "  layer.trainable = False\n",
    "\n",
    "# new_base_model.summary()\n",
    "\n",
    "# CONTINUE:\n",
    "# - build and compile Restnet50 model using this tutorial: https://github.com/ijosephp/apm_project/blob/main/Resnet50.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE RESNET50 MODEL AND SAVE IT LOCALLY\n",
    "\n",
    "new_base_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss',mode = 'min')\n",
    "\n",
    "# history = new_base_model.fit_generator(generator = train_ds, epochs=50,  validation_data = val_ds,callbacks=[callback])\n",
    "\n",
    "new_base_model.save(path_to_model) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
